using JuMP
using GLPK
using Ipopt
import Base.Iterators: product
import Random.shuffle

struct POMG
   Î³ # discount factor
   â„ # agents
   ğ’® # state space
   ğ’œ # joint action space
   ğ’ª # joint observation space
   T # transition function
   O # joint observation function
   R # joint reward function
end

struct SimpleGame
    Î³ # discount factor
    â„ # agents
    ğ’œ # joint action space
    R # joint reward function
end

struct NashEquilibrium 
end

struct ConditionalPlan
    a # action to take at root
    subplans # dictionary mapping observations to subplans
end
ConditionalPlan(a) = ConditionalPlan(a, Dict())
(Ï€::ConditionalPlan)() = Ï€.a
(Ï€::ConditionalPlan)(o) = Ï€.subplans[o]

SATED = 1
HUNGRY = 2
FEED = 1
IGNORE = 2
SING = 3
CRYING = true
QUIET = false
r_hungry = -10.0
r_feed = -5.0
r_sing = -0.5
p_become_hungry = 0.1
p_cry_when_hungry = 0.8
p_cry_when_not_hungry = 0.1
p_cry_when_hungry_in_sing = 0.9
discount_factor = 0.9
n_agents = 2
ordered_states() = [SATED, HUNGRY]
ordered_actions(i::Int) = [FEED, IGNORE, SING]
ordered_observations(i::Int) = [CRYING, QUIET]

function transition(s, a, s1)
   # Regardless, feeding makes the baby sated.
   if a[1] == FEED || a[2] == FEED
       if s1 == SATED
           return 1.0
       else
           return 0.0
       end
   else
       # If neither caretaker fed, then one of two things happens.
       # First, a baby that is hungry remains hungry.
       if s == HUNGRY
           if s1 == HUNGRY
               return 1.0
           else
               return 0.0
           end
       # Otherwise, it becomes hungry with a fixed probability.
       else
           probBecomeHungry = 0.5 #pomg.babyPOMDP.p_become_hungry
           if s1 == SATED
               return 1.0 - probBecomeHungry
           else
               return probBecomeHungry
           end
       end
   end
end

function joint_observation(a, s1, o)
   # If at least one caregiver sings, then both observe the result.
   if a[1] == SING || a[2] == SING
       # If the baby is hungry, then the caregivers both observe crying/silent together.
       if s1 == HUNGRY
           if o[1] == CRYING && o[2] == CRYING
               return p_cry_when_hungry_in_sing
           elseif o[1] == QUIET && o[2] == QUIET
               return 1.0 - p_cry_when_hungry_in_sing
           else
               return 0.0
           end
       # Otherwise the baby is sated, and the baby is silent.
       else
           if o[1] == QUIET && o[2] == QUIET
               return 1.0
           else
               return 0.0
           end
       end
   # Otherwise, the caregivers fed and/or ignored the baby.
   else
       # If the baby is hungry, then there's a probability it cries.
       if s1 == HUNGRY
           if o[1] == CRYING && o[2] == CRYING
               return p_cry_when_hungry
           elseif o[1] == QUIET && o[2] == QUIET
               return 1.0 - p_cry_when_hungry
           else
               return 0.0
           end
       # Similarly when it is sated.
       else
           if o[1] == CRYING && o[2] == CRYING
               return p_cry_when_not_hungry
           elseif o[1] == QUIET && o[2] == QUIET
               return 1.0 - p_cry_when_not_hungry
           else
               return 0.0
           end
       end
   end
end

function joint_reward(s, a)
   r = [0.0, 0.0]

   # Both caregivers do not want the child to be hungry.
   if s == HUNGRY
       r += [r_hungry, r_hungry]
   end

   # One caregiver prefers to feed.
   if a[1] == FEED
       r[1] += r_feed / 2.0
   elseif a[1] == SING
       r[1] += r_sing
   end

   # One caregiver prefers to sing.
   if a[2] == FEED
       r[2] += r_feed
   elseif a[2] == SING
       r[2] += r_sing / 2.0
   end

   # Note that caregivers only experience a cost if they do something.

   return r
end

function POMG_Init()
   return POMG(
       discount_factor,
       vec(collect(1:n_agents)),
       ordered_states(),
       [ordered_actions(i) for i in 1:n_agents],
       [ordered_observations(i) for i in 1:n_agents],
       (s, a, s1) -> transition(s, a, s1),
       (a, s1, o) -> joint_observation(a, s1, o),
       (s, a) -> joint_reward(s, a)
   )
end

# Policy evaluation  / Evaluation plan 

function lookahead(ğ’«::POMG, U, s, a)
   ğ’®, ğ’ª, T, O, R, Î³ = ğ’«.ğ’®, joint(ğ’«.ğ’ª), ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
   uâ€² = sum(T(s,a,sâ€²)*sum(O(a,sâ€²,o)*U(o,sâ€²) for o in ğ’ª) for sâ€² in ğ’®)
   return R(s,a) + Î³*uâ€²
end

# Pi is the vector of action

function evaluate_plan(ğ’«::POMG, Ï€, s)
   a = Tuple(Ï€i() for Ï€i in Ï€)
   U(o,sâ€²) = evaluate_plan(ğ’«, [Ï€i(oi) for (Ï€i, oi) in zip(Ï€,o)], sâ€²)
   return isempty(first(Ï€).subplans) ? ğ’«.R(s,a) : lookahead(ğ’«, U, s, a)
end

function utility(ğ’«::POMG, b, Ï€)
   u = [evaluate_plan(ğ’«, Ï€, s) for s in ğ’«.ğ’®]
   return sum(bs * us for (bs, us) in zip(b, u))
end

function tensorform(ğ’«::SimpleGame)
    â„, ğ’œ, R = ğ’«.â„, ğ’«.ğ’œ, ğ’«.R
    â„â€² = eachindex(â„)
    ğ’œâ€² = [eachindex(ğ’œ[i]) for i in â„]
    Râ€² = [R(a) for a in joint(ğ’œ)]
    return â„â€², ğ’œâ€², Râ€²
end

struct SimpleGamePolicy
    p # dictionary mapping actions to probabilities
    function SimpleGamePolicy(p::Base.Generator)
        return SimpleGamePolicy(Dict(p))
    end

    function SimpleGamePolicy(p::Dict)
        vs = collect(values(p))
        vs ./= sum(vs)
        return new(Dict(k => v for (k,v) in zip(keys(p), vs)))
    end
    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))
end
(Ï€i::SimpleGamePolicy)(ai) = get(Ï€i.p, ai, 0.0)
function (Ï€i::SimpleGamePolicy)()
    D = SetCategorical(collect(keys(Ï€i.p)), collect(values(Ï€i.p)))
    return rand(D)
end
joint(X) = vec(collect(product(X...)))
joint(Ï€, Ï€i, i) = [i == j ? Ï€i : Ï€j for (j, Ï€j) in enumerate(Ï€)]
function utility(ğ’«::SimpleGame, Ï€, i)
    ğ’œ, R = ğ’«.ğ’œ, ğ’«.R
    p(a) = prod(Ï€j(aj) for (Ï€j, aj) in zip(Ï€, a))
    return sum(R(a)[i]*p(a) for a in joint(ğ’œ))
end

function solve(M::NashEquilibrium, ğ’«::SimpleGame)
    â„, ğ’œ, R = tensorform(ğ’«)
    model = Model(Ipopt.Optimizer)
    @variable(model, U[â„])
    @variable(model, Ï€[i=â„, ğ’œ[i]] â‰¥ 0)
    @NLobjective(model, Min,
        sum(U[i] - sum(prod(Ï€[j,a[j]] for j in â„) * R[y][i]
        for (y,a) in enumerate(joint(ğ’œ))) for i in â„))
    @NLconstraint(model, [i=â„, ai=ğ’œ[i]],U[i] â‰¥ sum(
        prod(j==i ? (a[j]==ai ? 1.0 : 0.0) : Ï€[j,a[j]] for j in â„)
        * R[y][i] for (y,a) in enumerate(joint(ğ’œ))))
    @constraint(model, [i=â„], sum(Ï€[i,ai] for ai in ğ’œ[i]) == 1)
    optimize!(model)
    Ï€iâ€²(i) = SimpleGamePolicy(ğ’«.ğ’œ[i][ai] => value(Ï€[i,ai]) for ai in ğ’œ[i])
    return [Ï€iâ€²(i) for i in â„]
end

# Nash equilibrium 

struct POMGNashEquilibrium
   b # initial belief
   d # depth of conditional plans
end

depthOfPlan = 2
beliefInit = [1/3, 1/3, 1/3]
function POMGNashEquilibrium_Init()
    return POMGNashEquilibrium(
        beliefInit,
        depthOfPlan
    )
end

function create_conditional_plans(ğ’«, d)
   â„, ğ’œ, ğ’ª = ğ’«.â„, ğ’«.ğ’œ, ğ’«.ğ’ª
   Î  = [[ConditionalPlan(ai) for ai in ğ’œ[i]] for i in â„]
   for t in 1:d
      Î  = expand_conditional_plans(ğ’«, Î )
   end
   return Î 
end

function expand_conditional_plans(ğ’«, Î )
   â„, ğ’œ, ğ’ª = ğ’«.â„, ğ’«.ğ’œ, ğ’«.ğ’ª
   return [[ConditionalPlan(ai, Dict(oi => Ï€i for oi in ğ’ª[i])) for Ï€i in Î [i] for ai in ğ’œ[i]] for i in â„]
end

function solve(M::POMGNashEquilibrium, ğ’«::POMG)
   â„, Î³, b, d = ğ’«.â„, ğ’«.Î³, M.b, M.d
   Î  = create_conditional_plans(ğ’«, d)
   U = Dict(Ï€ => utility(ğ’«, b, Ï€) for Ï€ in joint(Î ))
   ğ’¢ = SimpleGame(Î³, â„, Î , Ï€ -> U[Ï€])
   Ï€ = solve(NashEquilibrium(), ğ’¢)
   return Tuple(argmax(Ï€i.p) for Ï€i in Ï€)
end

myPOMG = POMG_Init()
myNash = POMGNashEquilibrium_Init()
result = solve(myNash, myPOMG)
for (idx, i) in enumerate(result)
    # println(i)
    println("Agent ", idx)
    println(i)
end

# Tao 1 ham xuly kq vong for de println
# Print kq vao file de visualize